# -*- coding: utf-8 -*-
"""Explainable ML_exercise 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yGp3uTswX3XavS44vBrFmlWswUqmGN60
"""

import pandas as pd

# Read the CSV file
df = pd.read_csv("FBPS-ValidationData.csv", delimiter='\t')

# Make a copy of the data
df_copy = df.copy()
1
# Define a function to map birthpos to the desired output
def map_birthpos(birthpos):
    if birthpos == 1:
        return 1
    elif birthpos == df['birthpos'].iloc[-1]:
        return 3
    else:
        return 2

# Apply the mapping function to create a new column 'y'
df_copy['y'] = df_copy['birthpos'].map(map_birthpos)

# Select only probands with native language English
df_copy = df_copy[df_copy['engnat'] == 1]

# Drop unnecessary columns
columns_to_drop = ['submittime', 'country', 'dateload', 'source', 'screensize', 'introelapse', 'testelapse', 'endelapse']
df_copy = df_copy.drop(columns=columns_to_drop, axis=1)

# Display a preview of the processed data
print("Processed Data Preview:")
print(df_copy.head())  # Display the first few rows of the processed data

# Save the processed data to a new CSV file
df_copy.to_csv("processed_data.csv", index=False)

"""Creating a copy of the read data is crucial for several reasons:

Preservation of Original Data:
When you perform operations or modifications on a dataset in Python (or any programming language), changes are often applied in place. Creating a copy ensures that the original dataset remains intact and can be referred to or reused in its initial state if needed.

Avoiding Unintended Modifications:
If modifications are made directly to the original data without creating a copy, there's a risk of unintended changes. This could lead to data loss or irreversible alterations, making it difficult to trace back to the original dataset.

Iterative Analysis or Experimentation:
In data analysis or machine learning tasks, you might explore different operations or transformations. Working with a copy allows you to experiment without affecting the source data. You can compare results, revert to the original, or iterate through various modifications more confidently.

Parallel Processing and Multithreading:
In certain scenarios where parallel processing or multithreading is employed for analysis, having a separate copy for each thread ensures that each thread operates on its own set of data, avoiding conflicts or data corruption issues.

In summary, creating a copy of the dataset provides a safety net, enabling safe experimentation and analysis without compromising the integrity of the original data. It's a fundamental practice in data manipulation workflows to maintain data integrity and facilitate reproducibility in analyses.

The columns to delete include:

'submittime': Submission time information
'screensize': Screen size data
'introelapse', 'testelapse', 'endelapse': Time-related information during the test
These columns seem unrelated to the analysis or outcome based on the given task and can be removed to streamline the dataset.

the output data would include:

'y' Column Modification:

A new column ('y') created based on the conditions specified:
If 'birthpos' == 1, the corresponding 'y' value is set to 1.
If 'birthpos' is the same as the last value in the 'birthpos' column, the 'y' value is set to 3.
For all other 'birthpos' values, 'y' is set to 2.
Subset of Proband Data with Native English Language:

A filtered subset of the dataset that includes only entries where the 'engnat' column indicates native English speakers (where 'engnat' == 1).
These are the two primary output data components resulting from the modifications and filtering described in the task.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.preprocessing import LabelEncoder

# Assuming you have already loaded and processed the data into df_copy

# Encode categorical variables if needed
label_encoder = LabelEncoder()
df_copy['gender'] = label_encoder.fit_transform(df_copy['gender'])

# Define features and target variable
X = df_copy.drop(['y'], axis=1)
y = df_copy['y']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a RandomForestClassifier
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_classifier.predict(X_test)

# Print accuracy and confusion matrix
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)

# Reduce features using LinearDiscriminantAnalysis (LDA)
lda = LinearDiscriminantAnalysis()
X_train_lda = lda.fit_transform(X_train, y_train)
X_test_lda = lda.transform(X_test)

# Train a RandomForestClassifier on the reduced features
rf_classifier_lda = RandomForestClassifier(random_state=42)
rf_classifier_lda.fit(X_train_lda, y_train)

# Make predictions on the test set using reduced features
y_pred_lda = rf_classifier_lda.predict(X_test_lda)

# Print accuracy and confusion matrix with LDA
accuracy_lda = accuracy_score(y_test, y_pred_lda)
conf_matrix_lda = confusion_matrix(y_test, y_pred_lda)

print("\nResults with LinearDiscriminantAnalysis:")
print("Accuracy:", accuracy_lda)
print("Confusion Matrix:\n", conf_matrix_lda)

# Check if the dataset is balanced
class_counts = df_copy['y'].value_counts()
is_balanced = all(count >= 0.1 * len(df_copy) and count <= 0.9 * len(df_copy) for count in class_counts)

print("\nIs the dataset balanced?", is_balanced)

# Compare accuracy and balanced accuracy
balanced_accuracy = balanced_accuracy_score(y_test, y_pred)
balanced_accuracy_lda = balanced_accuracy_score(y_test, y_pred_lda)

print("\nAccuracy:", accuracy)
print("Balanced Accuracy:", balanced_accuracy)

print("\nAccuracy with LDA:", accuracy_lda)
print("Balanced Accuracy with LDA:", balanced_accuracy_lda)

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# Decision Tree
decision_tree = DecisionTreeClassifier(random_state=42)
decision_tree.fit(X_train, y_train)
dt_pred = decision_tree.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_pred)
print("Decision Tree Accuracy:", dt_accuracy)

# Random Forest
random_forest = RandomForestClassifier(random_state=42)
random_forest.fit(X_train, y_train)
rf_pred = random_forest.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_pred)
print("Random Forest Accuracy:", rf_accuracy)

# Gradient Boosting
gradient_boosting = GradientBoostingClassifier(random_state=42)
gradient_boosting.fit(X_train, y_train)
gb_pred = gradient_boosting.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)
print("Gradient Boosting Accuracy:", gb_accuracy)

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# Read the processed data
df = pd.read_csv("processed_data.csv")

# Features and target
X = df.drop(columns=['y'])  # Features
y = df['y']  # Target

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Decision Tree
decision_tree = DecisionTreeClassifier(random_state=42)
decision_tree.fit(X_train, y_train)

# Random Forest
random_forest = RandomForestClassifier(random_state=42)
random_forest.fit(X_train, y_train)

# Gradient Boosting
gradient_boosting = GradientBoostingClassifier(random_state=42)
gradient_boosting.fit(X_train, y_train)

# Get feature importances for Random Forest
rf_feature_importances = random_forest.feature_importances_

# Get feature importances for Gradient Boosting
gb_feature_importances = gradient_boosting.feature_importances_

# Zip feature importances with feature names for Random Forest
rf_feature_importances_zip = zip(X_train.columns, rf_feature_importances)

# Zip feature importances with feature names for Gradient Boosting
gb_feature_importances_zip = zip(X_train.columns, gb_feature_importances)

# Sort the feature importances by importance score (descending order)
rf_feature_importances_sorted = sorted(rf_feature_importances_zip, key=lambda x: x[1], reverse=True)
gb_feature_importances_sorted = sorted(gb_feature_importances_zip, key=lambda x: x[1], reverse=True)

# Print feature importances (Top-N features)
print("Random Forest Feature Importances:")
for feature, importance in rf_feature_importances_sorted[:10]:  # Display top 10 features
    print(f"{feature}: {importance}")

print("\nGradient Boosting Feature Importances:")
for feature, importance in gb_feature_importances_sorted[:10]:  # Display top 10 features
    print(f"{feature}: {importance}")

# Print cardinality of 'gender' and 'I sometimes ruin my jokes by laughing in the middle of them'
print(f"\nCardinality of 'gender': {df['gender'].nunique()}")
print(f"Cardinality of 'I sometimes ruin my jokes by laughing in the middle of them': "
      f"{len('I sometimes ruin my jokes by laughing in the middle of them')}")

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.preprocessing import MinMaxScaler

# Fit Linear Discriminant Analysis
lda = LinearDiscriminantAnalysis()
lda.fit(X_train, y_train)

# Get coefficients
lda_coefs = lda.coef_

# Normalize coefficients
scaler = MinMaxScaler()
lda_coefs_normalized = scaler.fit_transform(lda_coefs)

# Print normalized coefficients
print("Normalized Coefficients:")
print(lda_coefs_normalized)

# Calculate class representation in the model
class_representation = lda.priors_
print("\nClass Representation:")
print(class_representation)

# Combine coefficients with feature names
coef_with_names = list(zip(X_train.columns, lda_coefs_normalized[0]))

# Sort features based on weighted importance
sorted_features = sorted(coef_with_names, key=lambda x: abs(x[1]), reverse=True)

# Print weighted features sorted into an HTML table
print("\nWeighted Features (Top-N):")
print("<table>")
print("<tr><th>Feature</th><th>Weight</th></tr>")
for feature, weight in sorted_features[:10]:  # Display top 10 features
    print(f"<tr><td>{feature}</td><td>{weight}</td></tr>")
print("</table>")

# Filter out individuals who don’t insult people and don’t boss people around based on feature importance
non_insult_boss = [feat for feat, _ in sorted_features if 'insult' not in feat.lower() and 'boss' not in feat.lower()]
print("\nWho doesn’t insult people and doesn’t boss people around?")
print(non_insult_boss)

# Permutation Feature Importance calculates the importance of features by measuring the increase in the model's error when the values of that feature are randomly shuffled
# The advantage lies in its ability to evaluate feature importance without making assumptions about feature dependencies and is useful when dealing with complex models or when the relationship between features is nonlinear.

"""Permutation Feature Importance offers a significant advantage when assessing feature importance in machine learning models because:

Model Agnostic: Permutation Feature Importance is model agnostic, meaning it can be applied to any machine learning model regardless of its complexity or type (regression, classification, tree-based, neural networks, etc.). This flexibility allows for a consistent approach to evaluate feature importance across various models.

Nonlinearity and Feature Interactions: It accurately captures the effects of nonlinear relationships and feature interactions. Unlike some other methods that might assume linear relationships or specific interactions, permutation importance evaluates the impact of each feature by shuffling its values while keeping other features intact, making it suitable for complex datasets with intricate feature interactions.

Reduces Bias: By assessing the model's performance after permuting a specific feature, it determines the change in performance, indicating how much that feature contributes to the model's predictive power. This method helps reduce bias that might occur when assuming certain feature relationships.

Handles Correlated Features: It handles correlated features more effectively by individually assessing each feature's importance without assumptions about their relationships. This is particularly useful when dealing with multicollinearity, where multiple features might be correlated, ensuring a more accurate representation of individual feature importance.

Robustness: Permutation Feature Importance is robust against overfitting and tends to provide more reliable insights into feature importance, especially when the dataset has noise or redundant features.

Overall, Permutation Feature Importance offers a versatile, robust, and unbiased approach to evaluate the impact of each feature on the model's performance, making it a valuable tool for understanding feature contributions in complex machine learning models.
"""

# from sklearn.datasets import fetch_california_housing

# # Load the California housing dataset
# california_housing = fetch_california_housing()

# # Access the data and target from the dataset
# data = california_housing.data  # Features
# target = california_housing.target  # Target variable

# # Display some information about the dataset
# print(f"Shape of data: {data.shape}")
# print(f"Shape of target: {target.shape}")

!pip install pdpbox

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.ensemble import RandomForestRegressor

# Load the California housing dataset
california_housing = fetch_california_housing()

# Select only 100 data points
np.random.seed(42)
random_indices = np.random.choice(california_housing.data.shape[0], 100, replace=False)
data_100 = california_housing.data[random_indices]
target_100 = california_housing.target[random_indices]

# Feature names
feature_names = california_housing.feature_names

# Create DataFrame for easier handling
california_df = pd.DataFrame(data_100, columns=feature_names)

# RandomForestRegressor
rf_regressor = RandomForestRegressor(random_state=42)
rf_regressor.fit(data_100, target_100)

# Function to calculate average predictions for a feature
def calculate_pdp(feature_name, feature_range):
    predictions = []
    for val in feature_range:
        temp_df = california_df.copy()
        temp_df[feature_name] = val
        predictions.append(np.mean(rf_regressor.predict(temp_df)))
    return predictions

# Plot PDP of 'average number of rooms per household'
feature_name = 'AveRooms'
feature_range = np.linspace(california_df[feature_name].min(), california_df[feature_name].max(), num=50)
predictions = calculate_pdp(feature_name, feature_range)

plt.figure(figsize=(8, 6))
plt.plot(feature_range, predictions)
plt.xlabel(feature_name)
plt.ylabel('Predicted Median House Value')
plt.title(f'Partial Dependence Plot - {feature_name}')
plt.show()

# ... (similarly, create PDP plots for other features)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.ensemble import RandomForestRegressor

# Load the California housing dataset
california_housing = fetch_california_housing()

# Select only 100 data points
np.random.seed(42)
random_indices = np.random.choice(california_housing.data.shape[0], 100, replace=False)
data_100 = california_housing.data[random_indices]
target_100 = california_housing.target[random_indices]

# Feature names
feature_names = california_housing.feature_names

# Create DataFrame for easier handling
california_df = pd.DataFrame(data_100, columns=feature_names)

# RandomForestRegressor
rf_regressor = RandomForestRegressor(random_state=42)
rf_regressor.fit(data_100, target_100)

# Function to calculate ICE for a feature
def calculate_ice(feature_name, feature_range):
    ice_values = []
    for val in feature_range:
        temp_df = california_df.copy()
        temp_df[feature_name] = val
        ice_values.append(rf_regressor.predict(temp_df))
    return np.array(ice_values)

# Plot ICE of 'average number of household members' (AveOccup)
feature_name = 'AveOccup'
feature_range = np.linspace(california_df[feature_name].min(), california_df[feature_name].max(), num=50)
ice_values = calculate_ice(feature_name, feature_range)

plt.figure(figsize=(8, 6))
for i in range(ice_values.shape[1]):
    plt.plot(feature_range, ice_values[:, i], color='grey', alpha=0.4)
plt.xlabel(feature_name)
plt.ylabel('Predicted Median House Value')
plt.title(f'Individual Conditional Expectation Plot - {feature_name}')
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.ensemble import RandomForestRegressor

# Load the California housing dataset
california_housing = fetch_california_housing()

# Select only 100 data points
np.random.seed(42)
random_indices = np.random.choice(california_housing.data.shape[0], 100, replace=False)
data_100 = california_housing.data[random_indices]
target_100 = california_housing.target[random_indices]

# Feature names
feature_names = california_housing.feature_names

# Create DataFrame for easier handling
california_df = pd.DataFrame(data_100, columns=feature_names)

# RandomForestRegressor
rf_regressor = RandomForestRegressor(random_state=42)
rf_regressor.fit(data_100, target_100)

# Plot relationship between 'average number of bedrooms per household' and 'median house age'
feature_bedrooms = 'AveBedrms'
feature_age = 'HouseAge'

plt.figure(figsize=(8, 6))
plt.scatter(california_df[feature_bedrooms], california_df[feature_age], c=target_100, cmap='viridis')
plt.colorbar(label='Median House Value')
plt.xlabel('Average Number of Bedrooms per Household')
plt.ylabel('Median House Age')
plt.title('Relationship: Avg Bedrooms per Household vs. Median House Age')
plt.show()